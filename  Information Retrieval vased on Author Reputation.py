# -*- coding: utf-8 -*-
"""Проект SNA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zHq0C5GlC9MgwCkkw45hSAjq6bR57K5W

# **Достаем данные**
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import numpy as np

# user_list = []
# #user_list.append('username')
# for i in range(6):
#   url = "https://movies.stackexchange.com/users?page={}&tab=Reputation&filter=all".format(i)
#   response = requests.get(url)
#   soup = BeautifulSoup(response.text, "html.parser")
#   user_blocks = soup.find_all("div", {"class": "user-details"})
#   for user_block in user_blocks:
#       user_list.append('https://movies.stackexchange.com' + user_block.find("a")["href"])
# pd.DataFrame(user_list).to_csv("user_list.csv", index=False, header=False)

# user_list = pd.read_csv("user_list.csv", header=None)[0].tolist()

# user_url = user_list[0]
# question_list = []

# for page in range(1, 1000):
#     response = requests.get(user_url + f"?tab=answers&sort=votes&page={page}")
#     soup = BeautifulSoup(response.text, "html.parser")
#     question_blocks = soup.find_all("div", {"class": "s-post-summary--content"})
#     for question_block in question_blocks:
#         question_list.append('https://movies.stackexchange.com' +
#            question_block.find("a")["href"]
#         )
#     # если на странице нет вопросов, то выходим из цикла
#     if len(question_blocks) == 0:
#         break
#     # чтобы не забанили, ждем 1 секунду
#     time.sleep(1)

# pd.DataFrame(question_list).to_csv("question_list.csv", index=False, header=False)

# # Создание пустой таблицы для хранения данных
# #results = []

# for question_url in question_list[1101:]:
#     response = requests.get(question_url)
#     soup = BeautifulSoup(response.text, "html.parser")

#     # Извлечение данных из вопроса
# #    question_title = soup.find("a", {"class": "postcell"}).text
# #    print("Question:", question_title)

#     # Извлечение ответов на вопрос
#     answer_cells = soup.find_all("div", {"class": "post-layout"})
#     i = 0
#     for answer_cell in answer_cells:
#         try:
#             answer_text = answer_cell.find("div", {"class": "js-post-body"}).text.strip()
#             last_author = answer_cell.find_all("div", {"class": "user-details"})[-1]
#             author_name = last_author.find("a").text
#             reputation = last_author.find("span", {"class": "reputation-score"}).text
#             likes = answer_cell.find("div", {"class": "js-vote-count"}).text
#             print("Answer:", answer_text)
#             print("Author:", author_name)
#             print("Reputation:", reputation)
#             print("Likes:", likes)

#             # Добавление данных в таблицу
#             is_question = True if i == 0 else False
#             results.append((question_url, i, is_question, answer_text, author_name, reputation, likes))
#             i += 1
#             # Ожидание 1 секунды перед следующим запросом
#             time.sleep(1)
#         except Exception as e:
#             pass

# pd.DataFrame(results).to_csv("answers.csv", index=False, header=True)

data=pd.read_csv('/content/answers.csv')

"""# **Смотрим на данные и обрабатываем**"""

data.head(20)

#Изменили репутацию, сделали из нее обычные числа и формат float
data['5'] = data['5'].str.replace(',', '')
data['5'] = data['5'].apply(lambda x: float(x[:-1]) * 1000 if x.endswith('k') else float(x))

data.shape

"""# **Строим граф**"""

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
# создаем пустой граф
G = nx.Graph()

# проходимся по каждому вопросу и связываем ответчиков, которые на него отвечали
for question in set(data['0']):
    answers = data[data['0'] == question]
    for i, answer1 in answers.iterrows():
        for j, answer2 in answers.iterrows():
            if i != j:
                if answer1['4']!=answer2['4']:
                  if G.has_edge(answer1['4'], answer2['4']):
                    G[answer1['4']][answer2['4']]['weight'] += 1
                  else:
                    G.add_edge(answer1['4'], answer2['4'])
                    G[answer1['4']][answer2['4']]['weight'] = 1

#community detection
# рисуем граф синих узлов (узлов вопросов)
plt.figure(figsize=(10, 6))
nx.draw(G, with_labels=True, node_color='blue', alpha=0.5)
plt.show()

"""# **Описательные статистики графа**

**Центральности**
"""

# Вычисление различных мер центральности
degree_centrality = nx.degree_centrality(G)
closeness_centrality = nx.closeness_centrality(G)
betweenness_centrality = nx.betweenness_centrality(G)

# Вывод результатов
print("Degree Centrality:")
for node, centrality in degree_centrality.items():
    print(f"Node {node}: {centrality}")

print("\nCloseness Centrality:")
for node, centrality in closeness_centrality.items():
    print(f"Node {node}: {centrality}")

print("\nBetweenness Centrality:")
for node, centrality in betweenness_centrality.items():
    print(f"Node {node}: {centrality}")

# Меры центральности включают степень центральности (degree_centrality), близость центральности (closeness_centrality) и центральность посредничества (betweenness_centrality)

# Нахождение узла с наибольшей степенью центральности
max_degree_node = max(degree_centrality, key=degree_centrality.get)
print("Узел с наибольшей степенью центральности:")
print(f"Node: {max_degree_node}")
print(f"Degree Centrality: {degree_centrality[max_degree_node]}")

# Нахождение узла с наибольшей близостью центральности
max_closeness_node = max(closeness_centrality, key=closeness_centrality.get)
print("Узел с наибольшей близостью центральности:")
print(f"Node: {max_closeness_node}")
print(f"Closeness Centrality: {closeness_centrality[max_closeness_node]}")

# Нахождение узла с наибольшей центральностью посредничества
max_betweenness_node = max(betweenness_centrality, key=betweenness_centrality.get)
print("Узел с наибольшей центральностью посредничества:")
print(f"Node: {max_betweenness_node}")
print(f"Betweenness Centrality: {betweenness_centrality[max_betweenness_node]}")

"""**Диаметр**"""

# Вычисление диаметра графа
diameter = nx.diameter(G)
print(f"Диаметр графа: {diameter}")

"""**Плотность**"""

# Вычисление плотности графа
density = nx.density(G)
print(f"Плотность графа: {density}")

"""# **Устанавливаем нужные библиотеки**"""

pip install -U sentence-transformers

pip install node2vec

from node2vec import Node2Vec

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('sentence-transformers/paraphrase-TinyBERT-L6-v2')

"""# **Осуществляем подготовку входных параметров**"""

node2vec = Node2Vec(G, dimensions=100, walk_length=80, num_walks=15, workers=10)
graph_model = node2vec.fit(window=10, min_count=1, batch_words=4)
author_embeddings={}

for i in range(len(data)):
  author_name=data.iloc[i][4]
  author_embeddings[author_name]=graph_model.wv.get_vector(author_name)

#создаем пустые списки
question_text_embeddings=[]
answer_text_embeddings=[]
author_ratings=[]
question_ratings=[]
author_connection_embeddings=[]

#Сразу заполняем списки нужными нам значениями
y = []
question_text_embedding=[]
question_rating=0

for i in range(len(data)):
  if data.iloc[i][2]==True:
    question_rating = data.iloc[i][6]
    question_text = data.iloc[i][3]
    question_text_embedding=model.encode(question_text)
  else:
    answer_text = data.iloc[i][3]
    answer_text_embedding=model.encode(answer_text)
    answer_text_embeddings.append(answer_text_embedding)
    question_text_embeddings.append(question_text_embedding)
    author_ratings.append(data.iloc[i][5])
    question_ratings.append(question_rating)
    author_connection_embeddings.append(author_embeddings[data.iloc[i][4]])
    y.append(data.iloc[i][6])
    print(i) #просто чтобы отслеживать прогресс, так сказать

"""# **Обучаем модели**

## **Обучим модель на рейтингах**
"""

X=[]
for i in range(len(y)):
  X.append(np.array([author_ratings[i]]))
  X[i]=np.concatenate((X[i], np.array([question_ratings[i]])))

from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
regr = RandomForestRegressor(random_state=42)
regr.fit(X_train, y_train)

#Вывод предсказаний модели
y_pred = regr.predict(X_test)
print(y_pred)

score = regr.score(X_test, y_test)
print("Оценка модели на тестовых данных:", score)

"""## **Обучение модели на основе взаимосвязи авторов**"""

X=[]
for i in range(len(y)):
  X.append(author_connection_embeddings[i])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
regr = RandomForestRegressor(random_state=42)
regr.fit(X_train, y_train)

y_pred = regr.predict(X_test)
print(y_pred)

score = regr.score(X_test, y_test)
print("Оценка модели на тестовых данных:", score)

"""## **Обучение модели на основе взаимосвязи авторов и их рейтинга**"""

X=[]
for i in range(len(y)):
  X.append(np.array([author_ratings[i]]))
  X[i]=np.concatenate((X[i], author_connection_embeddings[i]))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
regr = RandomForestRegressor(random_state=42)
regr.fit(X_train, y_train)

#Вывод предсказаний модели
y_pred = regr.predict(X_test)
print(y_pred)

score = regr.score(X_test, y_test)
print("Оценка модели на тестовых данных:", score)

"""## **Общая модель, в которой есть все параметры**"""

X=[]
for i in range(len(y)):
  X.append(question_text_embeddings[i])
  X[i]=np.concatenate((X[i], answer_text_embeddings[i]))
  X[i]=np.concatenate((X[i], np.array([author_ratings[i]])))
  X[i]=np.concatenate((X[i], np.array([question_ratings[i]])))
  X[i]=np.concatenate((X[i], author_connection_embeddings[i]))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
regr = RandomForestRegressor(random_state=42)
regr.fit(X_train, y_train)

#Вывод предсказаний модели
y_pred = regr.predict(X_test)
print(y_pred)

score = regr.score(X_test, y_test)
print("Оценка модели на тестовых данных:", score)
#самая крутая модель